{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KathanrDave/ML-2024-Project-6-Decision_Makers/blob/master/Codes/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTJY24SobZBO"
      },
      "outputs": [],
      "source": [
        "# @title Imputation Code\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import linear_model\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "RSI_verticalJumpData_Season3 = '/content/drive/My Drive/ML/DataSet/dataset.csv'\n",
        "df=pd.read_csv(RSI_verticalJumpData_Season3)\n",
        "\n",
        "# From the dataframe only columns with numeric data types are selected in order to impute\n",
        "numeric_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "#Using random state for reproducibilty, max_iteration = 10, 10 decision trees for random forest regressor\n",
        "imp = IterativeImputer(max_iter=10, random_state=0, estimator=RandomForestRegressor(n_estimators=10, random_state=0))\n",
        "# Fitting the imputer\n",
        "imp.fit(numeric_df)\n",
        "# Applying the imputer to the dataframe\n",
        "imputed_numeric_df = pd.DataFrame(imp.transform(numeric_df), columns=numeric_df.columns)\n",
        "# Concating the datasets\n",
        "imputed_df = pd.concat([imputed_numeric_df, df.drop(columns=numeric_df.columns)], axis=1)\n",
        "\n",
        "# Make sure to preserve the order of the columns\n",
        "original_columns = df.columns\n",
        "imputed_df = imputed_df[original_columns]\n",
        "\n",
        "imputed_df.to_csv('/content/drive/My Drive/ML/ImputedDataset/ImputedDataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_SqzczSlTEe",
        "outputId": "94e0a6ac-164f-43f8-fb0a-28e8a0d9e042"
      },
      "outputs": [],
      "source": [
        "# @title Feature Scaling Code\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File paths\n",
        "input_csv_path = '/content/drive/My Drive/ML/ImputedDataset/ImputedDataset.csv'\n",
        "output_csv_path = '/content/drive/My Drive/ML/NormalizedDataset/NormalizedData.csv'\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(input_csv_path)\n",
        "\n",
        "# Extract athlete names column\n",
        "athlete_names = data.iloc[:, 0]  # Assuming athlete names are in the first column\n",
        "\n",
        "# Separate features (X)\n",
        "X = data.iloc[:, 1:]  # Exclude the first column which contains athlete names\n",
        "\n",
        "# Initialize Min-Max scaler with custom feature range (0 to 10)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "# Fit and transform the features\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Convert the normalized features back to a DataFrame\n",
        "X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n",
        "\n",
        "# Concatenate athlete names column and normalized features\n",
        "normalized_data = pd.concat([athlete_names, X_normalized_df], axis=1)\n",
        "\n",
        "# Save the normalized data to a new CSV file on Google Drive\n",
        "normalized_data.to_csv(output_csv_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LALN7WXeB_Nq",
        "outputId": "587e1199-2208-4b50-e025-fa8e1f0933cd"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your CSV file from Google Drive\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/My Drive/ML/NormalizedDataset/NormalizedData.csv')\n",
        "# print(\"Total number of features:\", data.shape[1] - 1)\n",
        "# Drop the 'Athlete' column from the DataFrame\n",
        "data = data.drop(columns='Athlete')\n",
        "\n",
        "# Create a synthetic target variable\n",
        "data['target'] = range(len(data))\n",
        "\n",
        "# Split your data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an XGBoost model\n",
        "from xgboost import XGBRegressor\n",
        "model = XGBRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "importance = model.feature_importances_\n",
        "\n",
        "# Sort features by importance\n",
        "sorted_idx = importance.argsort()\n",
        "\n",
        "# Print feature importance for non-zero importance features\n",
        "# print(\"Features with non-zero importance:\")\n",
        "# for i in sorted_idx:\n",
        "#     if importance[i] > 0:\n",
        "#         print(f\"{X.columns[i]}: {importance[i]}\")\n",
        "\n",
        "# Plot feature importance\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6)) # Adjust the figure size for better visibility\n",
        "plt.barh(X.columns[sorted_idx], importance[sorted_idx])\n",
        "plt.xlabel(\"XGBoost Feature Importance\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Function to get highly correlated features\n",
        "def get_feature_correlation(df, top_n=None, corr_method='spearman',\n",
        "                            remove_duplicates=True, remove_self_correlations=True):\n",
        "    corr_matrix_abs = df.corr(method=corr_method).abs()\n",
        "    corr_matrix_abs_us = corr_matrix_abs.unstack()\n",
        "    sorted_correlated_features = corr_matrix_abs_us \\\n",
        "        .sort_values(kind=\"quicksort\", ascending=False) \\\n",
        "        .reset_index()\n",
        "\n",
        "    if remove_self_correlations:\n",
        "        sorted_correlated_features = sorted_correlated_features[\n",
        "            (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n",
        "        ]\n",
        "\n",
        "    if remove_duplicates:\n",
        "        sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n",
        "\n",
        "    sorted_correlated_features.columns = ['Feature 1', 'Feature 2', 'Correlation (abs)']\n",
        "\n",
        "    if top_n:\n",
        "        return sorted_correlated_features[:top_n]\n",
        "\n",
        "    return sorted_correlated_features\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "# If your DataFrame is named differently, replace 'data' with the name of your DataFrame\n",
        "\n",
        "# Set up the matplotlib figure with a larger size\n",
        "fig, ax = plt.subplots(figsize=(22, 22))\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = data.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", annot=True, fmt=\".2f\", ax=ax, cbar_kws={\"shrink\": .5})\n",
        "\n",
        "# Set the labels and rotate them for better readability\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(),\n",
        "    rotation=45,\n",
        "    horizontalalignment='right',\n",
        ")\n",
        "ax.set_yticklabels(\n",
        "    ax.get_yticklabels(),\n",
        "    rotation=45,\n",
        "    horizontalalignment='right',\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Display the list of highly correlated features\n",
        "top_n = 20 # Adjust this to display more or fewer top correlated pairs\n",
        "highly_correlated_features = get_feature_correlation(data, top_n=top_n)\n",
        "print(\"Top\", top_n, \"highly correlated feature pairs:\")\n",
        "print(highly_correlated_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0kJhcNYXXkF",
        "outputId": "57cd8567-89fd-4c1d-e67f-443b39477277"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = '/content/drive/My Drive/ML/NormalizedDataset/NormalizedData.csv'\n",
        "\n",
        "# Load your CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# List of columns to keep\n",
        "columns_to_keep = ['Athlete', 'RSI.Mean', 'Respiratory.Rate', 'HRV', 'Sleep.Efficiency....', 'Sleep.Consistency', 'Sleep.Disturbances', 'Recovery']\n",
        "\n",
        "# Select only the specified columns\n",
        "selected_data = data[columns_to_keep]\n",
        "\n",
        "# Specify the path to save the new CSV file\n",
        "output_file_path = '/content/drive/My Drive/ML/ReducedDataset/SelectedData.csv'\n",
        "\n",
        "# Save the selected data to a new CSV file\n",
        "selected_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Data saved to {output_file_path}\")\n",
        "\n",
        "# Print the list of features selected excluding the 'Athlete' column\n",
        "features_selected = selected_data.columns.tolist()\n",
        "features_selected.remove('Athlete') # Remove 'Athlete' from the list\n",
        "print(\"Features selected:\", features_selected)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDQ5mZ2529Nn",
        "outputId": "c7653c17-d425-4f24-b0e9-edb82e6aa2fe"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install lime\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b5Jg7Smk3YLC",
        "outputId": "c9293b52-1c74-4feb-b375-3137ae421b98"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/SelectedData.csv')\n",
        "\n",
        "# Assuming all features are already scaled between 0 and 1\n",
        "# Select the features you want to use for clustering\n",
        "features = ['RSI.Mean', 'Respiratory.Rate', 'HRV', 'Sleep.Efficiency....', 'Sleep.Consistency', 'Sleep.Disturbances', 'Recovery']\n",
        "\n",
        "# Initialize a list to store silhouette scores for each number of clusters\n",
        "silhouette_scores = []\n",
        "\n",
        "# Loop over the desired number of clusters\n",
        "for n_clusters in range(2, 8):\n",
        "    # Apply Gaussian Mixture Clustering on the original data with the current number of clusters\n",
        "    gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
        "    gmm.fit(data[features])\n",
        "\n",
        "    # Predict the clusters\n",
        "    data['cluster'] = gmm.predict(data[features])\n",
        "\n",
        "    # Compute the silhouette score\n",
        "    silhouette_avg = silhouette_score(data[features], data['cluster'])\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # Print the silhouette score for the current number of clusters\n",
        "    print(f\"Silhouette Score for {n_clusters} clusters: {silhouette_avg}\")\n",
        "\n",
        "# Plot the silhouette scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(2, 8), silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the clusters for the best number of clusters (you might need to adjust this based on the silhouette scores)\n",
        "best_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2 # +2 because the loop starts at 2\n",
        "gmm = GaussianMixture(n_components=best_n_clusters, random_state=42)\n",
        "gmm.fit(data[features])\n",
        "data['cluster'] = gmm.predict(data[features])\n",
        "\n",
        "# Since we are not using PCA, we cannot directly visualize the clusters in a 2D space.\n",
        "# Instead, you might consider using other dimensionality reduction techniques or visualization methods suitable for high-dimensional data.\n",
        "\n",
        "# Print the names of the athletes along with their cluster assignments\n",
        "for index, row in data.iterrows():\n",
        "    print(f\"Athlete: {row['Athlete']}, Cluster: {row['cluster']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "acNtOd0T3Zhe",
        "outputId": "2b468e35-18d5-497e-b802-7bea279d0e50"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/SelectedData.csv')\n",
        "\n",
        "# Assuming all features are already scaled between 0 and 1\n",
        "# Select the features you want to use for clustering\n",
        "features = ['RSI.Mean', 'Respiratory.Rate', 'HRV', 'Sleep.Efficiency....', 'Sleep.Consistency', 'Sleep.Disturbances', 'Recovery']\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(data[features])\n",
        "\n",
        "# Initialize a list to store silhouette scores for each number of clusters\n",
        "silhouette_scores = []\n",
        "\n",
        "# Loop over the desired number of clusters\n",
        "for n_clusters in range(2, 8):\n",
        "    # Apply Gaussian Mixture Clustering on the reduced data with the current number of clusters\n",
        "    gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
        "    gmm.fit(data_pca)\n",
        "\n",
        "    # Predict the clusters\n",
        "    data['cluster'] = gmm.predict(data_pca)\n",
        "\n",
        "    # Compute the silhouette score\n",
        "    silhouette_avg = silhouette_score(data_pca, data['cluster'])\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # Print the silhouette score for the current number of clusters\n",
        "    print(f\"Silhouette Score for {n_clusters} clusters: {silhouette_avg}\")\n",
        "\n",
        "# Plot the silhouette scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(2, 8), silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the clusters for the best number of clusters (you might need to adjust this based on the silhouette scores)\n",
        "best_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2 # +2 because the loop starts at 2\n",
        "gmm = GaussianMixture(n_components=best_n_clusters, random_state=42)\n",
        "gmm.fit(data_pca)\n",
        "data['cluster'] = gmm.predict(data_pca)\n",
        "\n",
        "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=data['cluster'], cmap='viridis')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title(f'Gaussian Mixture Clustering of Athlete Data with {best_n_clusters} clusters')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Print the names of the athletes along with their cluster assignments\n",
        "for index, row in data.iterrows():\n",
        "    print(f\"Athlete: {row['Athlete']}, Cluster: {row['cluster']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "H4lvyCyR3q6x",
        "outputId": "1653920d-9a1d-4f38-d589-aef3ce00682f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/SelectedData.csv')\n",
        "\n",
        "# Assuming all features are already scaled between 0 and 1\n",
        "# Select the features you want to use for clustering\n",
        "features = ['RSI.Mean', 'Respiratory.Rate', 'HRV', 'Sleep.Efficiency....', 'Sleep.Consistency', 'Sleep.Disturbances', 'Recovery']\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(data[features])\n",
        "\n",
        "# Apply Gaussian Mixture Clustering on the reduced data\n",
        "gmm = GaussianMixture(n_components=3, random_state=42)\n",
        "gmm.fit(data_pca)\n",
        "\n",
        "# Predict the clusters\n",
        "data['cluster'] = gmm.predict(data_pca)\n",
        "\n",
        "\n",
        "# Initialize the list to store silhouette scores\n",
        "silhouette_scores = []\n",
        "\n",
        "# Compute the silhouette score\n",
        "silhouette_avg = silhouette_score(data_pca, data['cluster'])\n",
        "silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Now, you can print the silhouette score\n",
        "# print(silhouette_scores[0])\n",
        "\n",
        "# Set print options for better formatting\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "# Assuming data_pca is a numpy array\n",
        "# print(data_pca)\n",
        "\n",
        "# # Visualize the clusters\n",
        "# plt.scatter(data_pca[:, 0], data_pca[:, 1], c=data['cluster'], cmap='viridis')\n",
        "# plt.xlabel('First Principal Component')\n",
        "# plt.ylabel('Second Principal Component')\n",
        "# plt.title('Gaussian Mixture Clustering of Athlete Data')\n",
        "# plt.colorbar(label='Cluster')\n",
        "# plt.show()\n",
        "\n",
        "# Print the names of the athletes along with their cluster assignments\n",
        "# for index, row in data.iterrows():\n",
        "#     print(f\"Athlete: {row['Athlete']}, Cluster: {row['cluster']}\")\n",
        "\n",
        "# Note: The surrogate model is trained on the PCA-transformed data and the GMM cluster assignments\n",
        "surrogate_model = RandomForestClassifier(random_state=42)\n",
        "surrogate_model.fit(data_pca, data['cluster'])\n",
        "\n",
        "# Use LIME to explain the surrogate model\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(data_pca, feature_names=features, class_names=['Cluster 0', 'Cluster 1', 'Cluster 2'], verbose=True, mode='classification')\n",
        "\n",
        "# Assuming you want to explain the prediction for the first instance\n",
        "#  Assuming the index of the instance in data_pca is 9\n",
        "instance_index = 9\n",
        "\n",
        "# Access the athlete's name from the original dataset using the index\n",
        "athlete_name = data.loc[instance_index, 'Athlete']\n",
        "\n",
        "# Print the athlete's name\n",
        "print(f\"Athlete: {athlete_name}\")\n",
        "\n",
        "# Now, proceed with the LIME explanation as before\n",
        "exp = explainer.explain_instance(data_pca[instance_index], surrogate_model.predict_proba, num_features=5)\n",
        "explanations = exp.as_list()\n",
        "\n",
        "# Print each explanation with its index\n",
        "for i, explanation in enumerate(explanations, start=1):\n",
        "    print(f\"Explanation {i}: {explanation}\")\n",
        "\n",
        "\n",
        "# Visualize the explanation\n",
        "exp.show_in_notebook(show_table=True, show_all=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "V5NJbrEE41ux",
        "outputId": "624b9634-11f2-451a-e3af-b838d22017cf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/SelectedData.csv')\n",
        "\n",
        "# Assuming all features are already scaled between 0 and 1\n",
        "# Select the features you want to use for clustering\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "features = ['RSI.Mean', 'HRV', 'Recovery','Sleep.Consistency','Sleep.Efficiency....','Respiratory.Rate','Sleep.Disturbances']\n",
        "silhouette_scores = []\n",
        "# Apply Gaussian Mixture Clustering on the original data\n",
        "gmm = GaussianMixture(n_components=3, random_state=42)\n",
        "gmm.fit(data[features])\n",
        "\n",
        "# Predict the clusters\n",
        "data['cluster'] = gmm.predict(data[features])\n",
        "# Group the data by the 'cluster' column\n",
        "grouped_data = data.groupby('cluster')\n",
        "\n",
        "# Iterate over each group and print the athlete names\n",
        "# for cluster_id, group in grouped_data:\n",
        "#     print(f\"Cluster {cluster_id}:\")\n",
        "#     for index, row in group.iterrows():\n",
        "#         print(f\" Athlete: {row['Athlete']}\")\n",
        "#     print()\n",
        "\n",
        "    # Compute the silhouette score\n",
        "silhouette_avg = silhouette_score(data[features], data['cluster'])\n",
        "silhouette_scores.append(silhouette_avg)\n",
        "# print(silhouette_scores[0])\n",
        "\n",
        "\n",
        "# Train the surrogate model on the original data and the GMM cluster assignments\n",
        "surrogate_model = RandomForestClassifier(random_state=42)\n",
        "surrogate_model.fit(data[features], data['cluster'])\n",
        "\n",
        "# Prepare the data for LIME\n",
        "# Assuming there are no categorical features, but if there are, specify them here\n",
        "categorical_features = [] # Example: ['feature_name']\n",
        "\n",
        "# Create the LIME explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    data[features].values,\n",
        "    feature_names=features,\n",
        "    class_names=['Cluster 0', 'Cluster 1', 'Cluster 2'],\n",
        "    verbose=True,\n",
        "    mode='classification',\n",
        "    categorical_features=categorical_features # Specify categorical features if any\n",
        ")\n",
        "\n",
        "instance_index = 21\n",
        "# Access the athlete's name from the original dataset using the index\n",
        "athlete_name = data.loc[instance_index, 'Athlete']\n",
        "\n",
        "# Print the athlete's name\n",
        "print(f\"Athlete: {athlete_name}\")\n",
        "\n",
        "# Explain the prediction for the first instance\n",
        "instance = data[features].iloc[instance_index].values.reshape(1, -1)\n",
        "instance_reshaped = instance.reshape(1, -1) # Reshape to (1, 7)\n",
        "# Convert to 1D array\n",
        "instance_1d = np.squeeze(instance_reshaped)\n",
        "print(instance_1d)\n",
        "# Ensure the instance is correctly formatted\n",
        "# assert instance.shape[1] == len(features), \"The number of features in the instance does not match the expected number.\"\n",
        "\n",
        "# Explicitly handle categorical features if any\n",
        "# This step is crucial if your dataset contains categorical features\n",
        "# For demonstration, let's assume there are no categorical features\n",
        "# If there are, you would need to convert them to numerical values before explaining\n",
        "# print(instance)\n",
        "# Proceed with the explanation\n",
        "exp = explainer.explain_instance(instance_1d, surrogate_model.predict_proba, num_features=len(features))\n",
        "# Assuming exp.as_list() returns a list of explanations\n",
        "explanations = exp.as_list()\n",
        "\n",
        "# Print each explanation with its index\n",
        "for i, explanation in enumerate(explanations, start=1):\n",
        "    print(f\"Explanation {i}: {explanation}\")\n",
        "\n",
        "# Visualize the explanation\n",
        "exp.show_in_notebook(show_table=True, show_all=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8CZt22mR5NUo",
        "outputId": "a93885bd-a52f-4711-a34b-bc045c155925"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/SelectedData.csv')\n",
        "\n",
        "features = ['RSI.Mean', 'HRV', 'Recovery','Sleep.Consistency','Sleep.Efficiency....','Respiratory.Rate','Sleep.Disturbances']\n",
        "silhouette_scores = []\n",
        "# Apply Gaussian Mixture Clustering on the original data\n",
        "gmm = GaussianMixture(n_components=3, random_state=42)\n",
        "gmm.fit(data[features])\n",
        "\n",
        "# Predict the clusters\n",
        "data['cluster'] = gmm.predict(data[features])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Group the data by the 'cluster' column\n",
        "grouped_data = data.groupby('cluster')\n",
        "\n",
        "# Iterate over each group and print the athlete names\n",
        "# for cluster_id, group in grouped_data:\n",
        "#     print(f\"Cluster {cluster_id}:\")\n",
        "#     for index, row in group.iterrows():\n",
        "#         print(f\" Athlete: {row['Athlete']}\")\n",
        "#     print()\n",
        "\n",
        "#     # Compute the silhouette score\n",
        "silhouette_avg = silhouette_score(data[features], data['cluster'])\n",
        "silhouette_scores.append(silhouette_avg)\n",
        "# print(silhouette_scores[0])\n",
        "\n",
        "\n",
        "# Train the surrogate model on the original data and the GMM cluster assignments\n",
        "surrogate_model = RandomForestClassifier(random_state=42)\n",
        "surrogate_model.fit(data[features], data['cluster'])\n",
        "\n",
        "# Prepare the data for LIME\n",
        "# Assuming there are no categorical features, but if there are, specify them here\n",
        "categorical_features = [] # Example: ['feature_name']\n",
        "\n",
        "# Create the LIME explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    data[features].values,\n",
        "    feature_names=features,\n",
        "    class_names=['Cluster 0', 'Cluster 1', 'Cluster 2'],\n",
        "    verbose=True,\n",
        "    mode='classification',\n",
        "    categorical_features=categorical_features # Specify categorical features if any\n",
        ")\n",
        "# Loop through each row in the DataFrame\n",
        "for index, row in data.iterrows():\n",
        "    # Access the athlete's name from the row\n",
        "    athlete_name = row['Athlete']\n",
        "\n",
        "    # Print the athlete's name\n",
        "    print(f\"Athlete: {athlete_name}\")\n",
        "\n",
        "    # Extract the features for the current athlete\n",
        "    instance = row[features].values.reshape(1, -1)\n",
        "    instance_1d = np.squeeze(instance )\n",
        "    # print(instance_1d)\n",
        "    # Proceed with the explanation\n",
        "    exp = explainer.explain_instance(instance_1d, surrogate_model.predict_proba, num_features=len(features))\n",
        "\n",
        "    # Print each explanation with its index\n",
        "    # explanations = exp.as_list()\n",
        "    # for i, explanation in enumerate(explanations, start=1):\n",
        "    #     print(f\"Explanation {i}: {explanation}\")\n",
        "\n",
        "    # Visualize the explanation\n",
        "    exp.show_in_notebook(show_table=True, show_all=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xs44SnA5ot1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
